# -*- coding: utf-8 -*-
"""Agentic rag v2 uni account.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DehGa5XcAdi1HmE6AVPdpSx_6vJRYh6B

#Agentic RAG (Retrival agent_Langraph)
"""

# Install necessary libraries
!pip install pinecone langchain-core
!pip install -U bitsandbytes
!pip install -U transformers accelerate
!pip install -U langchain-huggingface
!pip install -q langchain langchain-community pypdf transformers accelerate
!pip install -qU langchain-groq
!pip install -U --quiet langgraph langchain-text-splitters
!pip install langchain_pinecone

# Import required modules from LangChain and other libraries
# from langchain.vectorstores import FAISS
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
# from google import genai
# from google.genai import types
from typing import List
from langchain.embeddings.base import Embeddings
from langchain.docstore.document import Document
from langchain_text_splitters import MarkdownHeaderTextSplitter
from typing import List
import os
import getpass
from langchain_huggingface import HuggingFaceEmbeddings
import torch

"""change path for pdf accordingly

Skip this if you dont need to change chunking etc
"""

# 4. Initialize Pinecone
from pinecone import Pinecone, ServerlessSpec
from langchain.vectorstores import Pinecone as LangchainPinecone
# Import the PyPDF loader from langchain_community
from langchain_community.document_loaders import PyPDFDirectoryLoader
# from langchain.embeddings import HuggingFaceEmbeddings
from langchain_pinecone import PineconeVectorStore

# Replace with your actual Pinecone API key and environment
pc = Pinecone(api_key="pcsk_3EU17t_PpRzgpdJTEUoVMKPs52bdFQweiQMtRMm48FFQ4hyrQXoQ2QUJ3zZBzewxC3TrC6")
os.environ['PINECONE_API_KEY'] = 'pcsk_3EU17t_PpRzgpdJTEUoVMKPs52bdFQweiQMtRMm48FFQ4hyrQXoQ2QUJ3zZBzewxC3TrC6'
# Define a function to load all PDF documents from a directory
def load_documents(data_path):
    return PyPDFDirectoryLoader(data_path).load()

# Define the path to the directory containing the PDF files and loading the PDF documents
pdfs_path="/content/drive/MyDrive/Clara Project/Clara docs"
pdfs=load_documents(pdfs_path)

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
# Splitting the loaded PDF documents into manageable text chunks
splits = text_splitter.split_documents(documents=pdfs)

# device = "cuda"
# embedding_model = HuggingFaceEmbeddings(
#     model_name="Alibaba-NLP/gte-Qwen2-1.5B-instruct",
#     model_kwargs={"trust_remote_code": True, "device": device},
#     encode_kwargs={"normalize_embeddings": True}
# )


from langchain.vectorstores import Pinecone as LangchainPinecone
from pinecone import Pinecone, ServerlessSpec
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_pinecone import PineconeVectorStore

index_name = "clara-qwen2-1p5b-index"
# # Qwen2-1.5B-instruct outputs 1536-dimensional vectors
# if index_name not in pc.list_indexes():
#     pc.create_index(index_name, dimension=1536, metric="cosine", spec=ServerlessSpec(cloud="aws", region="us-east-1"))

# 5. Upload documents to Pinecone using batching
vectordb = PineconeVectorStore.from_documents(documents=splits, index_name=index_name, embedding=embedding_model)
print(f"Number of chunks created: {len(splits)}")

from pinecone import Pinecone, ServerlessSpec
from langchain.vectorstores import Pinecone as LangchainPinecone
# Import the PyPDF loader from langchain_community
from langchain_community.document_loaders import PyPDFDirectoryLoader
pc = Pinecone(api_key="pcsk_3EU17t_PpRzgpdJTEUoVMKPs52bdFQweiQMtRMm48FFQ4hyrQXoQ2QUJ3zZBzewxC3TrC6")

for idx in pc.list_indexes():
    print(idx.name, "-", idx.status)

from langchain.vectorstores import Pinecone as LangchainPinecone
from langchain.embeddings import HuggingFaceEmbeddings
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore

# Re-initialize Pinecone client
pc = Pinecone(api_key="pcsk_3EU17t_PpRzgpdJTEUoVMKPs52bdFQweiQMtRMm48FFQ4hyrQXoQ2QUJ3zZBzewxC3TrC6")
os.environ['PINECONE_API_KEY'] = 'pcsk_3EU17t_PpRzgpdJTEUoVMKPs52bdFQweiQMtRMm48FFQ4hyrQXoQ2QUJ3zZBzewxC3TrC6'

device = "cuda"
embedding_model = HuggingFaceEmbeddings(
    model_name="Alibaba-NLP/gte-Qwen2-1.5B-instruct",
    model_kwargs={"trust_remote_code": True, "device": device, },
    encode_kwargs={"normalize_embeddings": True}
)

# Connect to existing index and create retriever
index_name = "clara-qwen2-1p5b-index"
vectordb = PineconeVectorStore(index_name= index_name,embedding=embedding_model)
retriever= vectordb.as_retriever(search_kwargs={"k": 4})

"""Following code for Agentic RAG structure"""

# Making a retriever tool that LLM will use if query is specific
from langchain.tools.retriever import create_retriever_tool

retriever_tool = create_retriever_tool(
    retriever,
    "retrieve_policies",
    "Search and return accurate information about NACCAS policies.",
)
# test tool
# retriever_tool.invoke({"query": "Is GED acceptable for admission?"})

"""Building the node and edge structure

"""

# Add groq key to use LLM
os.environ["GROQ_API_KEY"] = "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"

from langgraph.graph import MessagesState
from langchain.chat_models import init_chat_model

# LLM assigning
response_model = init_chat_model("groq:meta-llama/llama-4-maverick-17b-128e-instruct", temperature=0.5, max_tokens=512, timeout=None)

def generate_query_or_respond(state: MessagesState):
    """Call the model to generate a response based on the current state. Given
    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.
    """
    response = (
        response_model
        .bind_tools([retriever_tool]).invoke(state["messages"])
    )
    return {"messages": [response]}

"""Grade documents"""

from pydantic import BaseModel, Field
from typing import Literal

GRADE_PROMPT = (
    "You are a grader assessing relevance of a retrieved document to a user question. \n "
    "Here is the retrieved document: \n\n {context} \n\n"
    "Here is the user question: {question} \n"
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n"
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."
)


class GradeDocuments(BaseModel):
    """Grade documents using a binary score for relevance check."""

    binary_score: str = Field(
        description="Relevance score: 'yes' if relevant, or 'no' if not relevant"
    )


grader_model = init_chat_model("groq:meta-llama/llama-4-maverick-17b-128e-instruct", temperature=0.5, max_tokens=512, timeout=None)


def grade_documents(
    state: MessagesState,
) -> Literal["generate_answer", "rewrite_question"]:
    """Determine whether the retrieved documents are relevant to the question."""
    question = state["messages"][0].content
    context = state["messages"][-1].content

    prompt = GRADE_PROMPT.format(question=question, context=context)
    response = (
        grader_model
        .with_structured_output(GradeDocuments).invoke(
            [{"role": "user", "content": prompt}]
        )
    )
    score = response.binary_score

    if score == "yes":
        return "generate_answer"
    else:
        return "rewrite_question"

REWRITE_PROMPT = (
    "Look at the input and try to reason about the underlying semantic intent / meaning.\n"
    "Here is the initial question:"
    "\n ------- \n"
    "{question}"
    "\n ------- \n"
    "ONLY respond by Formulating only 1 improved question, nothing else"
)
# explicitly stated one improved question

def rewrite_question(state: MessagesState):
    """Rewrite the original user question."""
    messages = state["messages"]
    question = messages[0].content
    prompt = REWRITE_PROMPT.format(question=question)
    response = response_model.invoke([{"role": "user", "content": prompt}])
    return {"messages": [{"role": "user", "content": response.content}]}

GENERATE_PROMPT = (
    "You are an assistant for question-answering tasks regarding NACCAS policies. "
    "Use the following pieces of retrieved context to answer the question. "
    "If you don't know the answer, just say that you don't know. "
    "Use 25 sentences maximum and keep the answer concise.\n"
    "Question: {question} \n"
    "Context: {context}"
)


def generate_answer(state: MessagesState):
    """Generate an answer."""
    question = state["messages"][0].content
    context = state["messages"][-1].content
    prompt = GENERATE_PROMPT.format(question=question, context=context)
    response = response_model.invoke([{"role": "user", "content": prompt}])
    return {"messages": [response]}

from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.prebuilt import tools_condition

workflow = StateGraph(MessagesState)

# Define the nodes we will cycle between
workflow.add_node(generate_query_or_respond)
workflow.add_node("retrieve", ToolNode([retriever_tool]))
workflow.add_node(rewrite_question)
workflow.add_node(generate_answer)

workflow.add_edge(START, "generate_query_or_respond")

# Decide whether to retrieve
workflow.add_conditional_edges(
    "generate_query_or_respond",
    # Assess LLM decision (call `retriever_tool` tool or respond to the user)
    tools_condition,
    {
        # Translate the condition outputs to nodes in our graph
        "tools": "retrieve",
        END: END,
    },
)

# Edges taken after the `action` node is called.
workflow.add_conditional_edges(
    "retrieve",
    # Assess agent decision
    grade_documents,
)
workflow.add_edge("generate_answer", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

# Compile
graph = workflow.compile()

from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))

from langchain_core.messages import BaseMessage
import time

start_time = time.time()
for chunk in graph.stream(
    {
        "messages": [
            {
                "role": "user",
                "content": "A full-time student in a 1500-hour esthetics program (academic year of 900 clock hours, 30 weeks) fails to meet the minimum 70% cumulative grade average at the second evaluation period (900 clock hours, 30 weeks). The institution allows appeals for SAP determinations. What steps must the institution take, and what must its SAP policy include regarding the appeal process?",
            }
        ]
    }
):
    for node, update in chunk.items():
        print("Update from node", node)
        # Check if 'messages' key exists and the last item is a BaseMessage before pretty printing
        if "messages" in update and update["messages"] and isinstance(update["messages"][-1], BaseMessage):
            update["messages"][-1].pretty_print()
        else:
            # Otherwise, print the update dictionary
            print(update)
        print("\n\n")
time.sleep(2)
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Elapsed time: {elapsed_time} seconds")

start_time = time.time()
for chunk in graph.stream(
    {
        "messages": [
            {
                "role": "user",
                "content": "On which things the curriculum of cosmetology based on ?",
            }
        ]
    }
):
    for node, update in chunk.items():
        print("Update from node", node)
        update["messages"][-1].pretty_print()
        print("\n\n")
time.sleep(2)
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Elapsed time: {elapsed_time} seconds")

questions = [
    "What is the refund policy?",
    "What if one catalog used for several institutions/Instructions regarding using other institute catalog?",
    "Who qualifies as ATB (ability to benefit)?",
    "What would be the catalog requirement / what should be the requirements of the check list of the institute?",
    "Which components must include in distant policy or for institutes that provide distant education?",
    "On which things the curriculum of cosmetology based on?",
    "What should be the curriculum of the instructor trainer?",
    "What should be the substantive interaction for the distant education learning activities?",
    "What are the Requirements for Institutions that have Undergone a Disaster?",
    "If institute is being harmed by any natural disaster what thing should be notify to NACCAS?",
    "What are the exceptions for the instructor qualifications?",
    "What factors would be considered by institute for teacher student ratio?",
    "What does the collection policy requires?",
    "Can a institute charge the student for extra instructional charges what are there rules?",
    "What does the Internal Grievance Procedure Policy require from the institute?",
    "What is Teach-Out Policy in accordance with policy in accordance to iv.05 policy?",
    "What is the submission requirements of the Teach out policy?",
    "What is leave of absence policy (LOA)?",
    "What are the requirements of LOA?",
    "What is Definition of an Ability-to-Benefit Student?",
    "A school applying for NACCAS accreditation wants to advertise its status. Can it claim to be accredited by NACCAS or use the phrase candidate for accreditation in its promotional materials? Explain the policy's stance.",
    "A school named Global Academy of Cosmetology and Massage wants to use Global Academy on its exterior signage and GAC in its social media advertising. Is this permissible under the NACCAS policy? Provide details to support your answer.",
    "A school wishes to advertise a new pilot program called \"Advanced Esthetics\" that has not yet been approved by NACCAS. What specific requirements must the school follow to comply with the policy when advertising this program?",
    "Can a school named \"Elite Beauty Institute, LLC\" use its full name, including \"LLC,\" in its advertising materials? Why or why not?",
    "A school that advertises that 90% of our graduates are employed within six months of graduation in its promotional brochure must have verifiable documentation to substantiate the claim and ensure that the data is current, accurate and based on the NACCAS definition of placement. What must it ensure?",
    "A prospective student wishes to enroll in a cosmetology program at a NACCAS-accredited school. They provide a transcript showing completion of a two-year associate’s degree program. Does this satisfy the educational requirements for admission, and what documentation must the school maintain?",
    "A homeschooled student applies to a NACCAS-accredited school in a state that issues a secondary school completion credential for homeschoolers. What must the student provide to be eligible for enrollment, and what additional condition must the school meet?",
    "Can a student who is still enrolled in high school enroll in an associate’s degree program at a NACCAS-accredited institution without a high school diploma? Why or why not?",
    "A school accepts ability-to-benefit (ATB) students and uses a third-party test to assess their eligibility. What documentation must the school maintain for these students, and what alternative option does the policy provide for ATB students?",
    "A student enrolls in a NACCAS-accredited school under a training agreement with a government agency. The agreement specifies certain admission requirements that differ from the school’s standard policy. How should the school handle this student’s admission, and what documentation is required?",
    "An ATB student completes a 1000-hour program at a NACCAS-accredited school in 2023, having demonstrated their ability to benefit by completing 225 clock hours after enrollment. By what year must the school retain the student’s ATB-related records, and what specific records are required?",
    "A NACCAS-accredited school admits an ATB student into a 500-hour program without requiring a test or post-enrollment completion of 6 credit hours or 225 clock hours. Does this comply with the NACCAS Ability-to-Benefit Policy, and why or why not?",
    "An institution collects a $200 registration fee from an applicant before providing an enrollment agreement. Does this comply with the NACCAS Enrollment Agreement Requirements policy, and why or why not?",
    "A NACCAS-accredited institution submits an enrollment agreement to the Commission as part of its Institutional Self-Study (ISS). What additional document must accompany the enrollment agreement, and what is its purpose?",
    "An institution hires an instructor for an occupational associate degree program who lacks an associate degree but provides letters of recommendation from a former employer. The institution does not retain these letters for review. The program includes a syllabus missing the date of last review, and the externship agreement lacks evaluation criteria. Does this comply with the NACCAS Policy on Occupational Associate Degree Programs, and why or why not?",
    "What are the minimum requirements for a course syllabus in an occupational associate degree program, according to the NACCAS Policy on Occupational Associate Degree Programs?",
    "A NACCAS-accredited institution offers a 900-clock-hour occupational associate degree program in esthetics, including 300 hours of lecture, 300 hours of lab, and 300 hours of clinical work. How many semester credit hours would this program equate to, according to the clock-to-credit hour conversion in the NACCAS policy?",
    "What financial and enrollment information must an institution include in its disaster recovery plan under the NACCAS Policy on Disasters?",
    "A NACCAS-accredited institution’s campus is severely damaged by a hurricane, forcing it to cease teaching for two weeks, which is not listed in the catalog. What must the institution do to comply with the NACCAS Policy on Disasters, and to whom should it send the notification?",
    "A student requires extra instruction due to excessive absences but has not yet reached the completion date on their enrollment agreement. The institution charges the student for these hours and records the payment. What additional step must the institution take to comply with the NACCAS Policy on Extra Instructional Charges?",
    "A student in a 600-hour cosmetology program has exhausted all allowed absences within the contract period and requires an additional 30 hours of instruction to complete the program. The enrollment agreement specifies a charge of $10 per clock hour for extra instruction. Can the institution charge the student for these hours, and what must it ensure regarding the charges?",
    "A student enrolls in a 600-hour cosmetology program at a NACCAS-accredited institution in a state with no mandated refund policy. The student withdraws after completing 60 hours (10% of the program). According to the NACCAS Minimum Tuition Adjustment Schedule, what percentage of the total tuition does the student owe, and within what timeframe must the refund be issued?",
    "An institution offers an Electrology program in a state without mandated content. What must the curriculum include to comply with the NACCAS Minimum Curriculum Requirements Policy?",
    "A NACCAS-accredited institution offers a Hairdressing-only program in a state with no mandated curriculum content. What components of the Cosmetology curriculum can the institution exclude, and what must it still include?",
    "In the absence of state-mandated content, what curriculum components must an institution include for a Cosmetology program, according to the NACCAS Minimum Curriculum Requirements Policy?",
    "A NACCAS-accredited institution offers a 600-hour cosmetology program with 200 hours delivered via synchronous distance education. What must the institution ensure regarding substantive and regular interaction between students and instructors in this program?",
    "A NACCAS-accredited institution offers a 1000-hour cosmetology program and wants to include a state-authorized externship course. A student is selected for the externship at a licensed salon. What criteria must the institution use to select the student, and what examination must the student have completed?",
    "A student at a NACCAS-accredited institution wishes to file a formal grievance about an instructor’s conduct. The institution has a written grievance policy, but the student is unaware of it because it was not communicated at the program’s start. The institution provides complaint forms only upon request. Does this comply with the NACCAS Internal Grievance Procedure Policy, and what steps should the institution take to address the situation?",
    "A full-time student in a 48-credit-hour esthetics program (two 24-credit-hour terms) fails to achieve a 70% cumulative grade average at the midpoint of the first term (12 credit hours). The institution participates in Title IV programs and allows appeals for SAP determinations. What steps must the institution take, and what must its SAP policy include regarding the appeal process?",
    "A full-time student in a 1500-hour esthetics program (academic year of 900 clock hours, 30 weeks) fails to meet the minimum 70% cumulative grade average at the second evaluation period (900 clock hours, 30 weeks). The institution allows appeals for SAP determinations. What steps must the institution take, and what must its SAP policy include regarding the appeal process?"
]

!pip install python-docx

from docx import Document
from langgraph.graph import MessagesState
from langchain_core.messages import BaseMessage
import re
import time

# Load your questions

# Load the Word document
doc = Document("/content/Updated_Responses_GT_as_B.docx")

# Locate all 'A:' paragraph indices
a_paragraphs = [i for i, para in enumerate(doc.paragraphs) if para.text.strip().startswith("A:")]

# Function to extract final response from graph
def get_final_response(question, graph):
    last_message = None
    for chunk in graph.stream({"messages": [{"role": "user", "content": question}]}):
        for node, update in chunk.items():
            if "messages" in update and update["messages"]:
                last_message = update["messages"][-1]
    return last_message.content if last_message else "No response generated."

# Process each question and replace its corresponding 'A:' response
for i, question in enumerate(questions):
    print(f"Processing Q{i+1}: {question[:60]}...")
    try:
        response = get_final_response(question, graph)
        doc.paragraphs[a_paragraphs[i]].text = f"A: {response}"
    except Exception as e:
        doc.paragraphs[a_paragraphs[i]].text = f"A: ERROR - {str(e)}"
    # time.sleep()  # prevent rate limiting

# Save the updated document
doc.save("Updated_Responses_Final.docx")
print("✅ Done: Saved as 'Updated_Responses_Final.docx'")

!pip install bert-score python-docx pandas

from docx import Document
from bert_score import score
import pandas as pd

# --- Step 1: Load the document ---
doc = Document("/content/Updated_Responses_Final.docx")
paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]

# --- Step 2: Extract responses for A and B ---
model_a_responses = []
model_b_responses = []

for para in paragraphs:
    if para.startswith("A:"):
        cleaned_a = para[2:].replace('\n', ' ').replace('\r', ' ').strip()
        model_a_responses.append(" ".join(cleaned_a.split()))
    elif para.startswith("B:"):
        model_b_responses.append(para[2:].strip())

# --- Step 3: Verify response pairs ---
assert len(model_a_responses) == len(model_b_responses), "Uneven responses detected."
print(f"Found {len(model_a_responses)} response pairs.")

# --- Step 4: Compute BERTScore for each pair ---
P_list, R_list, F1_list = [], [], []

for i, (a, b) in enumerate(zip(model_a_responses, model_b_responses)):
    P, R, F1 = score([a], [b], lang='en', verbose=True)
    P_list.append(P.item())
    R_list.append(R.item())
    F1_list.append(F1.item())

# --- Step 5: Save results to CSV ---
df = pd.DataFrame({
    "Model_A_Response": model_a_responses,
    "Model_B_Response": model_b_responses,
    "Precision": P_list,
    "Recall": R_list,
    "F1_Score": F1_list
})

df.to_csv("agentic_basicRag_bert_score_results.csv", index=False)
print("Scores saved to 'agentic_basicRag_bert_score_results.csv'")
print(f"Average F1 Score: {sum(F1_list)/len(F1_list):.4f}")